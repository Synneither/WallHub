import sqlite3
import requests
import os
import logging
import hashlib
import time
import concurrent.futures
from datetime import datetime
from contextlib import contextmanager
from src.utils import is_valid_image


class DatabaseImageDownloader:
    """ä»æ•°æ®åº“ä¸­ä¸‹è½½å›¾ç‰‡çš„ä¸‹è½½å™¨"""
    
    def __init__(self, db_path, save_dir, source='all'):
        """
        åˆå§‹åŒ–æ•°æ®åº“å›¾ç‰‡ä¸‹è½½å™¨
        
        Args:
            db_path: SQLiteæ•°æ®åº“è·¯å¾„
            save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
            source: å›¾ç‰‡æº ('reddit', 'wallhaven', 'all')
        """
        self._setup_logging()
        self.logger = logging.getLogger('DatabaseImageDownloader')
        self.logger.info(f"ğŸš€ åˆå§‹åŒ–æ•°æ®åº“å›¾ç‰‡ä¸‹è½½å™¨... (æº: {source})")
        
        self.db_path = db_path
        self.save_dir = save_dir
        self.source = source
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        }
        self.request_timeout = 10
        self.download_timeout = 20
        self.sleep_time = 1
        self.max_workers = 5
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.save_dir, exist_ok=True)
        self.logger.info(f"ğŸ“ å›¾ç‰‡ä¿å­˜ç›®å½•: {self.save_dir}")
        
        self.logger.info("âœ… ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        log_filename = f"{log_dir}/database_downloader_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        logging.getLogger('requests').setLevel(logging.WARNING)
        logging.getLogger('urllib3').setLevel(logging.WARNING)

    @contextmanager
    def get_db_connection(self):
        """æ•°æ®åº“è¿æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = sqlite3.connect(self.db_path, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            self.logger.error(f"âŒ æ•°æ®åº“äº‹åŠ¡å›æ»š: {e}")
            raise
        finally:
            conn.close()

    def get_images_from_db(self):
        """ä»æ•°æ®åº“è·å–æ‰€æœ‰æœªä¸‹è½½çš„å›¾ç‰‡è®°å½•"""
        try:
            with self.get_db_connection() as conn:
                cursor = conn.cursor()
                
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='images'")
                if not cursor.fetchone():
                    self.logger.warning("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰imagesè¡¨ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                    return []
                
                # è·å–æ‰€æœ‰å›¾ç‰‡
                cursor.execute("SELECT id, name, url, hash FROM images")
                images = cursor.fetchall()
                
                self.logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å– {len(images)} æ¡å›¾ç‰‡è®°å½•")
                return [dict(row) for row in images]
        
        except sqlite3.Error as e:
            self.logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {e}")
            return []

    def generate_filename(self, image_hash, url):
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # æå–æ‰©å±•å
        extension = self._get_extension_from_url(url)
        return f"{image_hash}.{extension}"

    def _get_extension_from_url(self, url):
        """ä»URLè·å–æ–‡ä»¶æ‰©å±•å"""
        # ç§»é™¤æŸ¥è¯¢å‚æ•°
        path = url.split('?')[0].lower()
        
        if path.endswith('.jpg') or path.endswith('.jpeg'):
            return 'jpg'
        elif path.endswith('.png'):
            return 'png'
        elif path.endswith('.gif'):
            return 'gif'
        elif path.endswith('.webp'):
            return 'webp'
        elif path.endswith('.avif'):
            return 'avif'
        else:
            return 'jpg'  # é»˜è®¤ä½¿ç”¨jpg

    def download_image(self, image_data):
        """ä¸‹è½½å•ä¸ªå›¾ç‰‡"""
        url = image_data['url']
        image_hash = image_data['hash']
        filename = self.generate_filename(image_hash, url)
        filepath = os.path.join(self.save_dir, filename)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(filepath):
            self.logger.debug(f"â­ï¸ å›¾ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            return True
        
        try:
            self.logger.info(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½: {filename} <- {url}")
            response = requests.get(
                url,
                headers=self.headers,
                timeout=self.download_timeout,
                stream=True
            )
            response.raise_for_status()
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾ç‰‡
            content_type = response.headers.get('content-type', '')
            if not is_valid_image(response.content, content_type):
                self.logger.warning(f"âš ï¸ æ— æ•ˆçš„å›¾ç‰‡æ ¼å¼ï¼Œè·³è¿‡: {url}")
                return False
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            self.logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")
            time.sleep(self.sleep_time)  # é€Ÿç‡é™åˆ¶
            return True
        
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥: {url} - {e}")
            return False
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ–‡ä»¶é”™è¯¯: {filepath} - {e}")
            return False

    def run(self):
        """è¿è¡Œä¸‹è½½å™¨"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ¬ å¼€å§‹ä¸‹è½½æ•°æ®åº“ä¸­çš„å›¾ç‰‡")
        self.logger.info("=" * 60)
        
        # è·å–æ•°æ®åº“ä¸­çš„å›¾ç‰‡
        images = self.get_images_from_db()
        
        if not images:
            self.logger.warning("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰å›¾ç‰‡è®°å½•ï¼Œæ— æ³•ä¸‹è½½")
            return
        
        self.logger.info(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½ {len(images)} ä¸ªå›¾ç‰‡...")
        
        # å¹¶è¡Œä¸‹è½½
        downloaded_count = 0
        failed_count = 0
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.download_image, img): img for img in images}
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        downloaded_count += 1
                    else:
                        failed_count += 1
                except Exception as e:
                    self.logger.error(f"âŒ ä¸‹è½½çº¿ç¨‹å¼‚å¸¸: {e}")
                    failed_count += 1
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š ä¸‹è½½ç»Ÿè®¡")
        self.logger.info(f"âœ… æˆåŠŸ: {downloaded_count}")
        self.logger.info(f"â­ï¸  å·²å­˜åœ¨: {len(images) - downloaded_count - failed_count}")
        self.logger.info(f"âŒ å¤±è´¥: {failed_count}")
        self.logger.info(f"ğŸ“ ä¿å­˜ç›®å½•: {self.save_dir}")
        self.logger.info("=" * 60)


class RedditDatabaseDownloader(DatabaseImageDownloader):
    """Redditæ•°æ®åº“å›¾ç‰‡ä¸‹è½½å™¨"""
    
    def __init__(self, db_path, save_dir):
        super().__init__(db_path, save_dir, source='reddit')


class WallhavenDatabaseDownloader(DatabaseImageDownloader):
    """Wallhavenæ•°æ®åº“å›¾ç‰‡ä¸‹è½½å™¨"""
    
    def __init__(self, db_path, save_dir):
        super().__init__(db_path, save_dir, source='wallhaven')
