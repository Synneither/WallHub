import requests
import os
import logging
import hashlib
import time
import sqlite3
import re
import concurrent.futures
from datetime import datetime
from contextlib import contextmanager
from urllib.parse import urlencode
from config import WALLHAVEN_CONFIG
from src.utils import get_existing_hashes, is_valid_image


class WallhavenImageDownloader:
    def __init__(self):
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()

        self.logger = logging.getLogger('WallhavenImageDownloader')
        self.logger.info("ğŸš€ åˆå§‹åŒ– Wallhaven å›¾ç‰‡ä¸‹è½½å™¨...")

        self.save_dir = WALLHAVEN_CONFIG.get('save_dir')
        self.api_url = WALLHAVEN_CONFIG.get('api_url')
        self.api_key = WALLHAVEN_CONFIG.get('api_key')  # å¯é€‰
        self.max_images = WALLHAVEN_CONFIG.get('max_images')
        self.search_query = WALLHAVEN_CONFIG.get('search_query')
        self.categories = WALLHAVEN_CONFIG.get('categories')  # 1-general, 2-anime, 4-people (å¯ç»„åˆ)
        self.purity = WALLHAVEN_CONFIG.get('purity')  # 0-SFW, 1-sketchy, 2-NSFW (å¯ç»„åˆ)
        self.sorting = WALLHAVEN_CONFIG.get('sorting')  # date_added, relevance, random, views, favorites
        self.order = WALLHAVEN_CONFIG.get('order')  # desc, asc
        self.atleast = WALLHAVEN_CONFIG.get('atleast')  # ç‰¹å®šåˆ†è¾¨ç‡ï¼Œå¦‚ ['1920x1080', '2560x1440']
        self.ratios = WALLHAVEN_CONFIG.get('ratios')  # å®½é«˜æ¯”ï¼Œå¦‚ ['16x9', '21x9']
        self.max_pages = WALLHAVEN_CONFIG.get('max_pages')  # æœ€å¤§é¡µæ•°ï¼Œç”¨äºæ§åˆ¶ä¸‹è½½æ•°é‡
        self.default_pages = WALLHAVEN_CONFIG.get('default_pages')  # é»˜è®¤æ‰“å¼€å¤šå°‘é¡µä»¥è·å–æ›´å¤šå›¾ç‰‡
        self.topRange = WALLHAVEN_CONFIG.get('topRange')  # æ’åºèŒƒå›´ï¼Œå¦‚ '1d', '3d', '1w', '1M', '3M', '6M', '1y'
        # ä¸º Wallhaven API ä¼˜åŒ–çš„ headersï¼ˆç®€åŒ–ç‰ˆæœ¬ä»¥ç¡®ä¿å…¼å®¹æ€§ï¼‰
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        
        self.request_timeout = WALLHAVEN_CONFIG.get('request_timeout')
        self.download_timeout = WALLHAVEN_CONFIG.get('download_timeout')
        self.sleep_time = WALLHAVEN_CONFIG.get('sleep_time')
        self.db_path = WALLHAVEN_CONFIG.get('db_path')
        self.conn_pool = []
        self.max_connections = 5

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.save_dir, exist_ok=True)
        self.logger.info(f"ğŸ“ å›¾ç‰‡ä¿å­˜ç›®å½•: {self.save_dir}")

        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()
        # è·å–ç°æœ‰å›¾ç‰‡å“ˆå¸Œå€¼
        self.existing_hashes = get_existing_hashes(self.save_dir, self.db_path)
        self.logger.info(f"ğŸ” å‘ç° {len(self.existing_hashes)} ä¸ªå·²å­˜åœ¨çš„å›¾ç‰‡æ–‡ä»¶")
        existed_picture=existed_picture(self.db_path)
        self.logger.info(f"ğŸ” æ–‡ä»¶ä¸­æœ‰ {len(existed_picture)} ä¸ªå·²å­˜åœ¨çš„å›¾ç‰‡æ–‡ä»¶")
        self.logger.info("âœ… Wallhavenä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)

        # è®¾ç½®æ—¥å¿—æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        log_filename = f"{log_dir}/wallhaven_downloader_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            ]
        )

        # è®¾ç½®ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œé¿å…è¿‡å¤šè°ƒè¯•ä¿¡æ¯
        logging.getLogger('requests').setLevel(logging.WARNING)
        logging.getLogger('urllib3').setLevel(logging.WARNING)

    @contextmanager
    def get_db_connection(self):
        """æ•°æ®åº“è¿æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.conn_pool:
            conn = self.conn_pool.pop()
            self.logger.debug("â™»ï¸ ä»è¿æ¥æ± è·å–æ•°æ®åº“è¿æ¥")
        else:
            conn = sqlite3.connect(self.db_path, check_same_thread=False)
            conn.row_factory = sqlite3.Row
            self.logger.debug("ğŸ†• åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥")

        try:
            yield conn
            conn.commit()
            self.logger.debug("âœ… æ•°æ®åº“äº‹åŠ¡æäº¤æˆåŠŸ")
        except Exception as e:
            conn.rollback()
            self.logger.error(f"âŒ æ•°æ®åº“äº‹åŠ¡å›æ»š: {e}")
            raise
        finally:
            self.conn_pool.append(conn)
            self.logger.debug("ğŸ”™ æ•°æ®åº“è¿æ¥å½’è¿˜åˆ°è¿æ¥æ± ")

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS images (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    wallhaven_id TEXT NOT NULL UNIQUE,
                    name TEXT NOT NULL,
                    hash TEXT NOT NULL UNIQUE,
                    url TEXT NOT NULL UNIQUE,
                    source_url TEXT,
                    resolution TEXT,
                    stable INTEGER NOT NULL DEFAULT 1,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON images(url)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_hash ON images(hash)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_wallhaven_id ON images(wallhaven_id)')
            conn.commit()
            conn.close()
            self.logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        except sqlite3.Error as e:
            self.logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–é”™è¯¯: {e}")

    def insert_image(self, wallhaven_id, name, hash_value, url, source_url, resolution='unknown'):
        """æ’å…¥å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute(
                "INSERT INTO images (wallhaven_id, name, hash, url, source_url, resolution) VALUES (?, ?, ?, ?, ?, ?)",
                (wallhaven_id, name, hash_value, url, source_url, resolution)
            )
            conn.commit()
            conn.close()
            self.logger.info(f"ğŸ’¾ å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {name}")
            return True
        except sqlite3.IntegrityError as e:
            if "hash" in str(e):
                self.logger.warning(f"â­ï¸ å›¾ç‰‡hashå·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
            elif "url" in str(e):
                self.logger.warning(f"â­ï¸ å›¾ç‰‡URLå·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
            elif "wallhaven_id" in str(e):
                self.logger.warning(f"â­ï¸ Wallhaven IDå·²å­˜åœ¨ï¼Œè·³è¿‡: {wallhaven_id}")
            return False
        except sqlite3.Error as e:
            self.logger.error(f"âŒ æ’å…¥æ•°æ®åº“é”™è¯¯: {e}")
            return False

    def get_file_extension(self, content_type, url):
        """ä»å†…å®¹ç±»å‹æˆ–URLä¸­è·å–æ–‡ä»¶æ‰©å±•å"""
        # ä»å†…å®¹ç±»å‹è·å–æ‰©å±•å
        if 'image/jpeg' in content_type:
            return 'jpg'
        elif 'image/png' in content_type:
            return 'png'
        elif 'image/gif' in content_type:
            return 'gif'
        elif 'image/webp' in content_type:
            return 'webp'

        # ä»URLè·å–æ‰©å±•å
        if url.lower().endswith('.jpg') or url.lower().endswith('.jpeg'):
            return 'jpg'
        elif url.lower().endswith('.png'):
            return 'png'
        elif url.lower().endswith('.gif'):
            return 'gif'
        elif url.lower().endswith('.webp'):
            return 'webp'

        # é»˜è®¤ä½¿ç”¨ jpg
        return 'jpg'

    def calculate_image_hash(self, image_data):
        """è®¡ç®—å›¾ç‰‡çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(image_data).hexdigest()

    def generate_safe_filename(self, image_hash, file_extension):
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤å“ˆå¸Œå€¼ä¸­çš„éæ³•å­—ç¬¦
        safe_hash = re.sub(r'[^a-zA-Z0-9]', '', image_hash)

        # ç¡®ä¿æ‰©å±•åæœ‰æ•ˆ
        safe_extension = file_extension.lower()
        if safe_extension not in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
            safe_extension = 'jpg'  # é»˜è®¤ä½¿ç”¨jpgæ ¼å¼

        return f"{safe_hash}.{safe_extension}"

    def search_wallhaven(self, page=1, retries=3):
        """æœç´¢Wallhavenå¹¶è·å–å›¾ç‰‡æ•°æ®ï¼Œæ”¯æŒè‡ªåŠ¨é‡è¯•"""
        for attempt in range(retries):
            try:
                params = {
                    'page': page,
                    'categories': self.categories,
                    'purity': self.purity,
                    'sorting': self.sorting,
                    'order': self.order,
                }

                # æ·»åŠ å¯é€‰å‚æ•°
                if self.search_query:
                    params['q'] = self.search_query
                if self.api_key:
                    params['apikey'] = self.api_key
                if self.atleast:
                    params['atleast'] = self.atleast
                if self.ratios:
                    params['ratios'] = self.ratios
                if self.topRange and self.sorting == 'toplist':
                    params['topRange'] = self.topRange  
                self.logger.debug(f"ğŸ” è¯·æ±‚å‚æ•°: {params}")
                full_url = f"{self.api_url}?{urlencode(params)}"
                self.logger.info(f"ğŸ”— å®Œæ•´è¯·æ±‚åœ°å€: {full_url}")
                response = requests.get(
                    self.api_url,
                    params=params,
                    headers=self.headers,
                    timeout=self.request_timeout
                )

                self.logger.debug(f"ğŸ“¡ API å“åº”çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code != 200:
                    self.logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    self.logger.debug(f"å“åº”å†…å®¹: {response.text[:200]}")
                    
                    # 429 å¤ªå¤šè¯·æ±‚ï¼Œç­‰å¾…åé‡è¯•
                    if response.status_code == 429:
                        wait_time = 5 * (attempt + 1)
                        self.logger.warning(f"â³ é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    
                    # 5xx æœåŠ¡å™¨é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                    if 500 <= response.status_code < 600:
                        wait_time = 3 * (attempt + 1)
                        self.logger.warning(f"â³ æœåŠ¡å™¨é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        if attempt < retries - 1:
                            time.sleep(wait_time)
                            continue
                    
                    return None

                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                if not response.text:
                    self.logger.error("âŒ API è¿”å›ç©ºå“åº”")
                    if attempt < retries - 1:
                        wait_time = 2 * (attempt + 1)
                        self.logger.warning(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    return None

                try:
                    data = response.json()
                    self.logger.info(f"ğŸ“„ è·å–åˆ° {len(data.get('data', []))} ä¸ªå£çº¸")
                    return data
                except ValueError as json_error:
                    self.logger.error(f"âŒ JSONè§£æå¤±è´¥: {json_error}")
                    self.logger.debug(f"å“åº”å†…å®¹ï¼ˆå‰ 500 å­—ç¬¦ï¼‰: {response.text[:500]}")
                    if attempt < retries - 1:
                        wait_time = 2 * (attempt + 1)
                        self.logger.warning(f"â³ JSON è§£æå¤±è´¥ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    return None

            except requests.exceptions.Timeout:
                self.logger.warning(f"â³ è¯·æ±‚è¶…æ—¶ï¼ˆç¬¬ {attempt + 1}/{retries} æ¬¡å°è¯•ï¼‰")
                if attempt < retries - 1:
                    wait_time = 2 * (attempt + 1)
                    self.logger.warning(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                self.logger.error(f"âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼ˆè¯·æ±‚è¶…æ—¶ï¼‰")
                return None
                
            except requests.exceptions.ConnectionError as e:
                self.logger.warning(f"â³ ç½‘ç»œè¿æ¥é”™è¯¯ï¼ˆç¬¬ {attempt + 1}/{retries} æ¬¡å°è¯•ï¼‰: {e}")
                if attempt < retries - 1:
                    wait_time = 3 * (attempt + 1)
                    self.logger.warning(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                self.logger.error(f"âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼ˆç½‘ç»œè¿æ¥é”™è¯¯ï¼‰")
                return None
                
            except Exception as e:
                self.logger.error(f"âŒ æœç´¢Wallhavenå¤±è´¥ï¼ˆç¬¬ {attempt + 1}/{retries} æ¬¡å°è¯•ï¼‰: {e}")
                if attempt < retries - 1:
                    wait_time = 2 * (attempt + 1)
                    self.logger.warning(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                return None
        
        return None

    def get_unique_image_urls(self, target_count):
        """è·å–æŒ‡å®šæ•°é‡çš„å”¯ä¸€å›¾ç‰‡URL"""
        self.logger.info(f"ğŸ¯ å¼€å§‹è·å– {target_count} ä¸ªå”¯ä¸€å›¾ç‰‡URL...")

        unique_urls = []
        unique_wallhaven_ids = set()
        existing_urls = self.get_existing_urls()
        existing_ids = self.get_existing_wallhaven_ids()

        self.logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_urls)} ä¸ªå›¾ç‰‡è®°å½•")

        page = self.default_pages 
        max_pages = self.max_pages  # æœ€å¤šå°è¯•5é¡µ
        
        while len(unique_urls) < target_count and page <= max_pages:
            self.logger.info(f"ğŸ“¥ è·å–ç¬¬ {page} é¡µ...")

            data = self.search_wallhaven(page=page)
            if not data or 'data' not in data:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ›´å¤šæ•°æ®å¯è·å–")
                break

            items = data.get('data', [])
            if not items:
                self.logger.warning("âš ï¸ å½“å‰é¡µé¢æ²¡æœ‰å›¾ç‰‡")
                break

            for item in items:
                if len(unique_urls) >= target_count:
                    break

                try:
                    wallhaven_id = item.get('id')
                    path = item.get('path')  # é«˜æ¸…å£çº¸URL
                    
                    if not path or wallhaven_id in existing_ids or path in existing_urls:
                        continue

                    unique_urls.append((path, wallhaven_id, item))
                    unique_wallhaven_ids.add(wallhaven_id)
                    self.logger.debug(f"âœ… å‘ç°æ–°å›¾ç‰‡: {wallhaven_id}")

                except (KeyError, TypeError) as e:
                    self.logger.warning(f"âš ï¸ è§£æå›¾ç‰‡æ•°æ®å¤±è´¥: {e}")
                    continue

            self.logger.info(f"ğŸ“Š å½“å‰å”¯ä¸€URLæ•°é‡: {len(unique_urls)}/{target_count}")
            page += 1
            time.sleep(self.sleep_time)  # éµå®ˆé€Ÿç‡é™åˆ¶

        self.logger.info(f"âœ… URLè·å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€å›¾ç‰‡URL")
        return unique_urls[:target_count]

    def get_existing_urls(self):
        """ä»æ•°æ®åº“è·å–æ‰€æœ‰å·²å­˜åœ¨çš„å›¾ç‰‡URL"""
        existing_urls = set()
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT url FROM images")
            existing_urls = {row[0] for row in cursor.fetchall()}
        self.logger.debug(f"ğŸ“‹ ä»æ•°æ®åº“åŠ è½½ {len(existing_urls)} ä¸ªç°æœ‰URL")
        return existing_urls

    def get_existing_wallhaven_ids(self):
        """ä»æ•°æ®åº“è·å–æ‰€æœ‰å·²å­˜åœ¨çš„Wallhaven ID"""
        existing_ids = set()
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT wallhaven_id FROM images")
            existing_ids = {row[0] for row in cursor.fetchall()}
        self.logger.debug(f"ğŸ“‹ ä»æ•°æ®åº“åŠ è½½ {len(existing_ids)} ä¸ªç°æœ‰Wallhaven ID")
        return existing_ids

    def download_image_optimized(self, url, wallhaven_id, item_data):
        """ä¼˜åŒ–åçš„ä¸‹è½½æ–¹æ³•"""
        try:
            # å‘é€è¯·æ±‚
            response = requests.get(
                url,
                headers=self.headers,
                stream=True,
                timeout=self.download_timeout
            )
            response.raise_for_status()

            # éªŒè¯å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'image' not in content_type:
                self.logger.warning(f"âš ï¸ éå›¾ç‰‡å†…å®¹ç±»å‹: {content_type} - {url}")
                return False

            # è·å–æ–‡ä»¶æ‰©å±•å
            file_extension = self.get_file_extension(content_type, url)

            # è¯»å–å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
            image_data = response.content
            
            # éªŒè¯å›¾ç‰‡æœ‰æ•ˆæ€§
            if not is_valid_image(image_data, content_type):
                self.logger.warning(f"âš ï¸ æ— æ•ˆçš„å›¾ç‰‡æ•°æ®: {url}")
                return False

            image_hash = self.calculate_image_hash(image_data)

            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            filename = self.generate_safe_filename(image_hash, file_extension)

            # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
            os.makedirs(self.save_dir, exist_ok=True)

            # æ„é€ ä¿å­˜è·¯å¾„
            save_path = os.path.join(self.save_dir, filename)

            # ä¿å­˜å›¾ç‰‡
            with open(save_path, 'wb') as f:
                f.write(image_data)

            # è·å–åˆ†è¾¨ç‡ä¿¡æ¯
            resolution = item_data.get('resolution', 'unknown')
            source_url = item_data.get('short_url', url)

            # ä¿å­˜åˆ°æ•°æ®åº“
            self.insert_image(wallhaven_id, filename, image_hash, url, source_url, resolution)

            # è®°å½•æˆåŠŸä¿¡æ¯
            self.logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {wallhaven_id} -> {filename}")
            return True

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ç½‘ç»œé”™è¯¯: {url} - {e}")
        except OSError as e:
            self.logger.error(f"âŒ æ–‡ä»¶ç³»ç»Ÿé”™è¯¯: {url} - {e}")
        except Exception as e:
            self.logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {url} - {e}")

        return False

    def mark_missing_images_unstable(self):
        """æ‰«æä¿å­˜ç›®å½•æ–‡ä»¶ï¼Œè‹¥æ•°æ®åº“è®°å½•çš„å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨åˆ™å°†å…¶ stable è®¾ä¸º 0"""
        self.logger.info("ğŸ” æ£€æŸ¥æ•°æ®åº“è®°å½•ä¸æœ¬åœ°æ–‡ä»¶ä¸€è‡´æ€§...")
        try:
            files = {f for f in os.listdir(self.save_dir) if os.path.isfile(os.path.join(self.save_dir, f))}
        except Exception as e:
            self.logger.error(f"âŒ æ— æ³•è®¿é—®ä¿å­˜ç›®å½•: {e}")
            return 0

        updated = 0
        try:
            with self.get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT id, name, stable FROM images")
                rows = cursor.fetchall()
                for row in rows:
                    row_id = row['id']
                    name = row['name']
                    stable = row['stable']
                    if name not in files and stable != 0:
                        cursor.execute("UPDATE images SET stable = 0 WHERE id = ?", (row_id,))
                        updated += 1
            if updated:
                self.logger.info(f"âš ï¸ æ ‡è®° {updated} æ¡æ•°æ®åº“è®°å½•ä¸º unstable (stable=0)")
            else:
                self.logger.info("âœ… æ•°æ®åº“ä¸­çš„å›¾ç‰‡æ–‡ä»¶å‡å­˜åœ¨ï¼Œæ— éœ€æ›´æ–°")
        except sqlite3.Error as e:
            self.logger.error(f"âŒ æ›´æ–°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        return updated

    def run(self):
        """è¿è¡Œä¸‹è½½ä»»åŠ¡"""
        self.logger.info("ğŸ¬ å¼€å§‹è¿è¡ŒWallhavenä¸‹è½½ä»»åŠ¡...")
        # é‡æ–°åŠ è½½ç°æœ‰çš„hashå’ŒURLé›†åˆ
        self.existing_hashes = get_existing_hashes(self.save_dir, self.db_path)
        self.logger.info(f"ğŸ” é‡æ–°åŠ è½½ {len(self.existing_hashes)} ä¸ªç°æœ‰å›¾ç‰‡å“ˆå¸Œ")

        # å°†æ•°æ®åº“ä¸ç£ç›˜æ–‡ä»¶åŒæ­¥ï¼šæ ‡è®°ç¼ºå¤±çš„å›¾ç‰‡ä¸º unstable
      #  updated_count = self.mark_missing_images_unstable()
      #  if updated_count:
      #      self.logger.info(f"ğŸ”§ å…±æ ‡è®° {updated_count} æ¡è®°å½•ä¸º unstable")

        # è·å–å”¯ä¸€å›¾ç‰‡URL
        target_count = self.max_images
        image_urls = self.get_unique_image_urls(target_count)

        if len(image_urls) < target_count:
            self.logger.warning(f"âš ï¸ åªæ‰¾åˆ° {len(image_urls)} ä¸ªå”¯ä¸€å›¾ç‰‡ï¼Œç›®æ ‡ä¸º {target_count} ä¸ª")
        else:
            self.logger.info(f"âœ… æˆåŠŸæ‰¾åˆ° {len(image_urls)} ä¸ªå”¯ä¸€å›¾ç‰‡")

        # å¹¶å‘ä¸‹è½½
        self.logger.info("ğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½å›¾ç‰‡...")
        processed = 0
        successful_downloads = 0

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = {
                executor.submit(self.download_image_optimized, url, wallhaven_id, item_data): url 
                for url, wallhaven_id, item_data in image_urls
            }
            for future in concurrent.futures.as_completed(futures):
                url = futures[future]
                processed += 1
                try:
                    if future.result():
                        successful_downloads += 1
                        self.logger.info(f"âœ… è¿›åº¦: {successful_downloads}/{len(image_urls)}")
                    else:
                        self.logger.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥æˆ–è·³è¿‡: {url}")
                except Exception as e:
                    self.logger.error(f"âŒ ä¸‹è½½å¼‚å¸¸: {e}")

                # æ¯10ä¸ªè¿›åº¦æŠ¥å‘Šä¸€æ¬¡
                if processed % 10 == 0:
                    self.logger.info(f"ğŸ“Š å¤„ç†è¿›åº¦: {processed}/{len(image_urls)}")

        # æœ€ç»ˆç»Ÿè®¡
        total_in_db = len(self.get_existing_urls())
        self.logger.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸä¸‹è½½ {successful_downloads} ä¸ªå”¯ä¸€å›¾ç‰‡")
        self.logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­ç°æœ‰ {total_in_db} ä¸ªå›¾ç‰‡è®°å½•")

        # æ€§èƒ½ç»Ÿè®¡
        success_rate = (successful_downloads / len(image_urls)) * 100 if image_urls else 0
        self.logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
