import re
import requests
import os
import logging
from datetime import datetime
from config import REDDIT_CONFIG
from src.utils import get_existing_hashes, extract_image_url, is_valid_image
import hashlib
import time
import concurrent.futures
import sqlite3
from contextlib import contextmanager
from src.utils import existed_picture
class RedditImageDownloader:
    def __init__(self):
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()

        self.logger = logging.getLogger('RedditImageDownloader')
        self.logger.info("ğŸš€ åˆå§‹åŒ– Reddit å›¾ç‰‡ä¸‹è½½å™¨...")

        self.save_dir = REDDIT_CONFIG['save_dir']
        self.reddit_url = REDDIT_CONFIG['reddit_url']
        self.max_posts = REDDIT_CONFIG['max_posts']
        self.headers = REDDIT_CONFIG['headers']
        self.request_timeout = REDDIT_CONFIG['request_timeout']
        self.download_timeout = REDDIT_CONFIG['download_timeout']
        self.sleep_time = REDDIT_CONFIG['sleep_time']
        self.db_path = REDDIT_CONFIG['db_path']
        self.conn_pool = []
        self.after = REDDIT_CONFIG['after']  # ç”¨äºåˆ†é¡µçš„afterå‚æ•°
        self.max_connections = 5
        self.max_images = REDDIT_CONFIG['max_images']
        # æœç´¢è¶…æ—¶ä¸æ— è¿›å±•é™åˆ¶
        self.max_search_seconds = REDDIT_CONFIG.get('max_search_seconds', 300)
        self.max_empty_batches = REDDIT_CONFIG.get('max_empty_batches', 5)

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.save_dir, exist_ok=True)
        self.logger.info(f"ğŸ“ å›¾ç‰‡ä¿å­˜ç›®å½•: {self.save_dir}")
        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()

        # è·å–ç°æœ‰å›¾ç‰‡å“ˆå¸Œå€¼
        self.existing_hashes = get_existing_hashes(self.save_dir, self.db_path)
        self.logger.info(f"ğŸ” å‘ç° {len(self.existing_hashes)} ä¸ªå·²å­˜åœ¨çš„å›¾ç‰‡æ–‡ä»¶")
        self.existed_picture=existed_picture(self.db_path)
        self.logger.info(f"ğŸ” æ–‡ä»¶ä¸­æœ‰ {len(self.existed_picture)} ä¸ªå›¾ç‰‡")

        self.logger.info("âœ… ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)

        # è®¾ç½®æ—¥å¿—æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        log_filename = f"{log_dir}/reddit_downloader_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            ]
        )

        # è®¾ç½®ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œé¿å…è¿‡å¤šè°ƒè¯•ä¿¡æ¯
        logging.getLogger('requests').setLevel(logging.WARNING)
        logging.getLogger('urllib3').setLevel(logging.WARNING)
        logging.getLogger('aiohttp').setLevel(logging.WARNING)

    @contextmanager
    def get_db_connection(self):
        """æ•°æ®åº“è¿æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.conn_pool:
            conn = self.conn_pool.pop()
            self.logger.debug("â™»ï¸ ä»è¿æ¥æ± è·å–æ•°æ®åº“è¿æ¥")
        else:
            conn = sqlite3.connect(self.db_path, check_same_thread=False)
            conn.row_factory = sqlite3.Row
            self.logger.debug("ğŸ†• åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥")

        try:
            yield conn
            conn.commit()
            self.logger.debug("âœ… æ•°æ®åº“äº‹åŠ¡æäº¤æˆåŠŸ")
        except Exception as e:
            conn.rollback()
            self.logger.error(f"âŒ æ•°æ®åº“äº‹åŠ¡å›æ»š: {e}")
            raise
        finally:
            self.conn_pool.append(conn)
            self.logger.debug("ğŸ”™ æ•°æ®åº“è¿æ¥å½’è¿˜åˆ°è¿æ¥æ± ")

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS images (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    hash TEXT NOT NULL UNIQUE,
                    url TEXT NOT NULL UNIQUE,
                    stable INTEGER NOT NULL DEFAULT 1,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_url ON images(url)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_hash ON images(hash)')
            conn.commit()
            conn.close()
            self.logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        except sqlite3.Error as e:
            self.logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–é”™è¯¯: {e}")

    def insert_image(self, name, hash_value, url):
        """æ’å…¥å›¾ç‰‡ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("INSERT INTO images (name, hash, url) VALUES (?, ?, ?)", (name, hash_value, url))
            conn.commit()
            conn.close()
            self.logger.info(f"ğŸ’¾ å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {name}")
            return True
        except sqlite3.IntegrityError as e:
            if "hash" in str(e):
                self.logger.warning(f"â­ï¸ å›¾ç‰‡hashå·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
            elif "url" in str(e):
                self.logger.warning(f"â­ï¸ å›¾ç‰‡URLå·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
            return False
        except sqlite3.Error as e:
            self.logger.error(f"âŒ æ’å…¥æ•°æ®åº“é”™è¯¯: {e}")
            return False

    def get_file_extension(self, content_type, url):
        """ä»å†…å®¹ç±»å‹æˆ–URLä¸­è·å–æ–‡ä»¶æ‰©å±•å"""
        # ä»å†…å®¹ç±»å‹è·å–æ‰©å±•å
        if 'image/jpeg' in content_type:
            return 'jpg'
        elif 'image/png' in content_type:
            return 'png'
        elif 'image/gif' in content_type:
            return 'gif'
        elif 'image/webp' in content_type:
            return 'webp'

        # ä»URLè·å–æ‰©å±•å
        if url.lower().endswith('.jpg') or url.lower().endswith('.jpeg'):
            return 'jpg'
        elif url.lower().endswith('.png'):
            return 'png'
        elif url.lower().endswith('.gif'):
            return 'gif'
        elif url.lower().endswith('.webp'):
            return 'webp'

        # é»˜è®¤ä½¿ç”¨ jpg
        return 'jpg'

    def calculate_image_hash(self, image_data):
        """è®¡ç®—å›¾ç‰‡çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(image_data).hexdigest()

    def generate_safe_filename(self, image_hash, file_extension):
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤å“ˆå¸Œå€¼ä¸­çš„éæ³•å­—ç¬¦
        safe_hash = re.sub(r'[^a-zA-Z0-9]', '', image_hash)

        # ç¡®ä¿æ‰©å±•åæœ‰æ•ˆ
        safe_extension = file_extension.lower()
        if safe_extension not in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
            safe_extension = 'jpg'  # é»˜è®¤ä½¿ç”¨jpgæ ¼å¼

        return f"{safe_hash}.{safe_extension}"

    def generate_filename(self, image_hash, file_extension):
        """ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åï¼Œæ ¼å¼ä¸º: å“ˆå¸Œå€¼.æ‰©å±•å"""
        return f"{image_hash}.{file_extension}"

    def get_unique_image_urls(self, target_count):
        """è·å–æŒ‡å®šæ•°é‡çš„å”¯ä¸€å›¾ç‰‡URL"""
        self.logger.info(f"ğŸ¯ å¼€å§‹è·å– {target_count} ä¸ªå”¯ä¸€å›¾ç‰‡URL...")

        unique_urls = []
        processed_posts = 0
        after = self.after
       # self.logger.info(f"ğŸ” after ä¸º {after if after else 0}")

        existing_urls = self.get_existing_urls()
        self.logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_urls)} ä¸ªå›¾ç‰‡è®°å½•")
       # existed_picture=src.utils.existed_picture(self.db_path)
       # self.logger.info(f"ğŸ“Š æ–‡ä»¶ä¸­å·²æœ‰ {len(existed_picture)} ä¸ªå›¾ç‰‡æ–‡ä»¶")

        # è¶…æ—¶/æ— è¿›å±•æ§åˆ¶
        start_time = time.time()
        empty_batch_counter = 0
        max_search_seconds = getattr(self, 'max_search_seconds', 300)
        max_empty_batches = getattr(self, 'max_empty_batches', 5)

        def process_post_batch(posts_batch):
            """å¹¶è¡Œå¤„ç†ä¸€æ‰¹å¸–å­"""
            batch_urls = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = {}
                for child in posts_batch:
                    if len(batch_urls) >= target_count - len(unique_urls):
                        break

                    flair = child['data'].get('link_flair_text', '')
                    if flair and ('Desktop' in flair or 'æ¡Œé¢' in flair):
                        permalink = child['data']['permalink']
                        full_url = f"https://www.reddit.com{permalink}"
                        future = executor.submit(self.fetch_post_image_url, full_url)
                        futures[future] = full_url
                        self.logger.debug(f"ğŸ” æäº¤å¸–å­å¤„ç†ä»»åŠ¡: {full_url}")

                for future in concurrent.futures.as_completed(futures):
                    url = futures[future]
                    try:
                        image_url = future.result(timeout=10)
                        if image_url and image_url not in existing_urls:
                            batch_urls.append(image_url)
                            existing_urls.add(image_url)
                            self.logger.debug(f"âœ… å‘ç°æ–°å›¾ç‰‡URL: {image_url}")
                        else:
                            self.logger.debug(f"â­ï¸ è·³è¿‡é‡å¤æˆ–æ— æ•ˆURL: {url}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å¤„ç†å¸–å­å¤±è´¥: {url} - {e}")

            return batch_urls

        batch_count = 0
        while len(unique_urls) < self.max_images:
            batch_count += 1
            self.logger.info(f"ğŸ“¥ è·å–ç¬¬ {batch_count} æ‰¹å¸–å­...")

            api_url = f"https://www.reddit.com/r/Animewallpaper/.json?limit={target_count}"
            if after:
                api_url += f"&after={after}"

            try:
                response = requests.get(api_url, headers=self.headers)
                if response.status_code != 200:
                    self.logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    break

                data = response.json()
                posts = data['data']['children']
                self.logger.info(f"ğŸ“„ è·å–åˆ° {len(posts)} ä¸ªå¸–å­")

                if not posts:
                    self.logger.warning("âš ï¸ æ²¡æœ‰æ›´å¤šå¸–å­å¯è·å–")
                    break

                batch_size = 10
                prev_count = len(unique_urls)
                for i in range(0, len(posts), batch_size):
                    batch = posts[i:i+batch_size]
                    batch_urls = process_post_batch(batch)
                    unique_urls.extend(batch_urls)
                    self.logger.info(f"ğŸ“Š å½“å‰å”¯ä¸€URLæ•°é‡: {len(unique_urls)}/{self.max_images}")

                    if len(unique_urls) >= self.max_images:
                        self.logger.info("âœ… å·²è¾¾åˆ°ç›®æ ‡URLæ•°é‡")
                        break

                # æ£€æŸ¥æ˜¯å¦æœ‰è¿›å±•
                if len(unique_urls) == prev_count:
                    empty_batch_counter += 1
                    self.logger.info(f"âš ï¸ æœªåœ¨å½“å‰æ‰¹æ¬¡æ‰¾åˆ°æ–°å›¾ç‰‡ï¼ˆè¿ç»­ {empty_batch_counter}/{max_empty_batches} æ¬¡ï¼‰")
                else:
                    empty_batch_counter = 0

                # å¦‚æœè¿ç»­å¤šä¸ªæ‰¹æ¬¡æ— è¿›å±•ï¼Œåˆ™é€€å‡º
                if empty_batch_counter >= max_empty_batches:
                    self.logger.warning(f"âš ï¸ è¿ç»­ {max_empty_batches} ä¸ªæ‰¹æ¬¡æ²¡æœ‰æ–°å›¾ç‰‡ï¼Œåœæ­¢æœç´¢")
                    break

                # æ£€æŸ¥æ—¶é—´è¶…æ—¶
                elapsed = time.time() - start_time
                if elapsed >= max_search_seconds:
                    self.logger.warning(f"â±ï¸ æœç´¢è¶…æ—¶ï¼ˆ{elapsed:.1f}sï¼‰ï¼Œåœæ­¢æœç´¢")
                    break

                after = data['data'].get('after')
                self.logger.info(f"ğŸ” after ä¸º {after if after else 0}")
                if not after:
                    self.logger.warning("âš ï¸ å·²åˆ°è¾¾å¸–å­åˆ—è¡¨æœ«å°¾")
                    break

            except Exception as e:
                self.logger.error(f"âŒ è·å–å¸–å­åˆ—è¡¨å¤±è´¥: {e}")
                break

        self.logger.info(f"âœ… URLè·å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€å›¾ç‰‡URL")
        return unique_urls[:target_count]

    def fetch_post_image_url(self, post_url):
        """è·å–å•ä¸ªå¸–å­çš„å›¾ç‰‡URL"""
        try:
            self.logger.debug(f"ğŸŒ è·å–å¸–å­å†…å®¹: {post_url}")
            response = requests.get(post_url + ".json", headers=self.headers, timeout=8)
            if response.status_code == 200:
                image_url = extract_image_url(response.json())
                if image_url:
                    self.logger.debug(f"âœ… æˆåŠŸæå–å›¾ç‰‡URL: {image_url}")
                return image_url
            else:
                self.logger.warning(f"âš ï¸ å¸–å­è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ è·å–å¸–å­å›¾ç‰‡URLå¤±è´¥: {post_url} - {e}")
        return None

    def get_existing_urls(self):
        """ä»æ•°æ®åº“è·å–æ‰€æœ‰å·²å­˜åœ¨çš„å›¾ç‰‡URL"""
        existing_urls = set()
        with self.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT url FROM images")
            existing_urls = {row[0] for row in cursor.fetchall()}
        self.logger.debug(f"ğŸ“‹ ä»æ•°æ®åº“åŠ è½½ {len(existing_urls)} ä¸ªç°æœ‰URL")
        return existing_urls

    def is_likely_duplicate(self, image_url):
        """åŸºäºURLç‰¹å¾åˆ¤æ–­å›¾ç‰‡æ˜¯å¦å¯èƒ½é‡å¤"""
        # æ–¹æ³•1: æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦å·²å­˜åœ¨ï¼ˆå¿«é€Ÿä½†ä¸å®Œå…¨å‡†ç¡®ï¼‰
        filename = image_url.split('/')[-1].split('?')[0]
        potential_path = os.path.join(self.save_dir, filename)
        if os.path.exists(potential_path):
            return True
        return False

    def rate_limit_delay(self):
        """æ§åˆ¶è¯·æ±‚é¢‘ç‡"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        if elapsed < self.min_request_interval:
            sleep_time = self.min_request_interval - elapsed + random.uniform(0.1, 0.5)
            print(f"â³ è¯·æ±‚é—´éš”æ§åˆ¶: ç­‰å¾… {sleep_time:.1f} ç§’")
            time.sleep(sleep_time)
        self.last_request_time = time.time()

    def is_valid_image_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æŒ‡å‘æœ‰æ•ˆå›¾ç‰‡"""
        # æ£€æŸ¥URLæ‰©å±•å
        valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        if not any(url.lower().endswith(ext) for ext in valid_extensions):
            return False

        # æ£€æŸ¥URLæ ¼å¼ï¼ˆi.redd.itæ˜¯Redditçš„å›¾ç‰‡CDNï¼‰
        if 'i.redd.it' not in url:
            return False

        return True

    def download_image_optimized(self, url):
        """ä¼˜åŒ–åçš„ä¸‹è½½æ–¹æ³•"""
        try:
            # å‘é€è¯·æ±‚
            response = requests.get(
                url,
                headers=self.headers,
                stream=True,
                timeout=self.download_timeout
            )
            response.raise_for_status()

            # éªŒè¯å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if 'image' not in content_type:
                self.logger.warning(f"âš ï¸ éå›¾ç‰‡å†…å®¹ç±»å‹: {content_type} - {url}")
                return False

            # è·å–æ–‡ä»¶æ‰©å±•å
            file_extension = self.get_file_extension(content_type, url)

            # è¯»å–å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
            image_data = response.content
            image_hash = self.calculate_image_hash(image_data)

            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            filename = self.generate_safe_filename(image_hash, file_extension)

            # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
            os.makedirs(self.save_dir, exist_ok=True)

            # æ„é€ ä¿å­˜è·¯å¾„
            save_path = os.path.join(self.save_dir, filename)

            # è®°å½•è·¯å¾„ä¿¡æ¯
            self.logger.debug(f"ğŸ’¾ ä¿å­˜è·¯å¾„: {save_path}")

            # ä¿å­˜å›¾ç‰‡
            with open(save_path, 'wb') as f:
                f.write(image_data)
            self.insert_image(filename, image_hash, url)
            # è®°å½•æˆåŠŸä¿¡æ¯
            self.logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {url} -> {filename}")
            return True

        except requests.exceptions.RequestException as e:
            self.logger.error(f"âŒ ç½‘ç»œé”™è¯¯: {url} - {e}")
        except OSError as e:
            self.logger.error(f"âŒ æ–‡ä»¶ç³»ç»Ÿé”™è¯¯: {url} - {e}")
        except Exception as e:
            self.logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {url} - {e}")

        return False

    def mark_missing_images_unstable(self):
        """æ‰«æä¿å­˜ç›®å½•æ–‡ä»¶ï¼Œè‹¥æ•°æ®åº“è®°å½•çš„å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨åˆ™å°†å…¶ stable è®¾ä¸º 0"""
        self.logger.info("ğŸ” æ£€æŸ¥æ•°æ®åº“è®°å½•ä¸æœ¬åœ°æ–‡ä»¶ä¸€è‡´æ€§...")
        try:
            files = {f for f in os.listdir(self.save_dir) if os.path.isfile(os.path.join(self.save_dir, f))}
        except Exception as e:
            self.logger.error(f"âŒ æ— æ³•è®¿é—®ä¿å­˜ç›®å½•: {e}")
            return 0

        updated = 0
        try:
            with self.get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT id, name, stable FROM images")
                rows = cursor.fetchall()
                for row in rows:
                    row_id = row['id']
                    name = row['name']
                    stable = row['stable']
                    if name not in files and stable != 0:
                        cursor.execute("UPDATE images SET stable = 0 WHERE id = ?", (row_id,))
                        updated += 1
            if updated:
                self.logger.info(f"âš ï¸ æ ‡è®° {updated} æ¡æ•°æ®åº“è®°å½•ä¸º unstable (stable=0)")
            else:
                self.logger.info("âœ… æ•°æ®åº“ä¸­çš„å›¾ç‰‡æ–‡ä»¶å‡å­˜åœ¨ï¼Œæ— éœ€æ›´æ–°")
        except sqlite3.Error as e:
            self.logger.error(f"âŒ æ›´æ–°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        return updated

    def run(self):
        """è¿è¡Œä¸‹è½½ä»»åŠ¡"""
        self.logger.info("ğŸ¬ å¼€å§‹è¿è¡Œä¸‹è½½ä»»åŠ¡...")
        # é‡æ–°åŠ è½½ç°æœ‰çš„hashå’ŒURLé›†åˆ
        self.existing_hashes = get_existing_hashes(self.save_dir, self.db_path)
        self.logger.info(f"ğŸ” é‡æ–°åŠ è½½ {len(self.existing_hashes)} ä¸ªç°æœ‰å›¾ç‰‡å“ˆå¸Œ")

        # å°†æ•°æ®åº“ä¸ç£ç›˜æ–‡ä»¶åŒæ­¥ï¼šæ ‡è®°ç¼ºå¤±çš„å›¾ç‰‡ä¸º unstable
      #  updated_count = self.mark_missing_images_unstable()
      #  if updated_count:
      #      self.logger.info(f"ğŸ”§ å…±æ ‡è®° {updated_count} æ¡è®°å½•ä¸º unstable")

        # è·å–å”¯ä¸€å›¾ç‰‡URL
        target_count = self.max_posts
        max_image = self.max_images
        image_urls = self.get_unique_image_urls(target_count)

        if len(image_urls) < target_count:
            self.logger.warning(f"âš ï¸ åªæ‰¾åˆ° {len(image_urls)} ä¸ªå”¯ä¸€å›¾ç‰‡ï¼Œç›®æ ‡ä¸º {max_image} ä¸ª")
        else:
            self.logger.info(f"âœ… æˆåŠŸæ‰¾åˆ° {len(image_urls)} ä¸ªå”¯ä¸€å›¾ç‰‡")

        # å¹¶å‘ä¸‹è½½
        self.logger.info("ğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½å›¾ç‰‡...")
        processed = 0
        successful_downloads = 0

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = {executor.submit(self.download_image_optimized, url): url for url in image_urls}
            for future in concurrent.futures.as_completed(futures):
                url = futures[future]
                processed += 1
                try:
                    if future.result():
                        successful_downloads += 1
                        self.logger.info(f"âœ… è¿›åº¦: {successful_downloads}/{len(image_urls)} - {url}")
                    else:
                        self.logger.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥æˆ–è·³è¿‡: {url}")
                except Exception as e:
                    self.logger.error(f"âŒ ä¸‹è½½å¼‚å¸¸: {e} - {url}")

                # æ¯10ä¸ªè¿›åº¦æŠ¥å‘Šä¸€æ¬¡
                if processed % 10 == 0:
                    self.logger.info(f"ğŸ“Š å¤„ç†è¿›åº¦: {processed}/{len(image_urls)}")

        # æœ€ç»ˆç»Ÿè®¡
        total_in_db = len(self.get_existing_urls())
        self.logger.info(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸä¸‹è½½ {successful_downloads} ä¸ªå”¯ä¸€å›¾ç‰‡")
        self.logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­ç°æœ‰ {total_in_db} ä¸ªå›¾ç‰‡è®°å½•")

        # æ€§èƒ½ç»Ÿè®¡
        success_rate = (successful_downloads / len(image_urls)) * 100 if image_urls else 0
        self.logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")